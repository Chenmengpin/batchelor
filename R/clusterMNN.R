#' Cluster-based MNN
#'
#' Perform MNN correction based on cluster centroids, 
#' using the corrected centroid coordinates to correct the per-cell expression values 
#' with a variable bandwidth Gaussian kernel.
#'
#' @inheritParams fastMNN
#' @param clusters A list of length equal to \code{...} containing the assigned cluster for each cell in each batch.
#' If \code{NULL}, this is generated by k-means clustering with centers equal to the root of the number of cells in each batch.
#' 
#' @return
#' For \code{clusterMNN}, 
#' a \linkS4class{SingleCellExperiment} containing per-cell expression values where each row is a gene and each column is a cell.
#' This has the same format as the output of \code{\link{fastMNN}}
#' but with an additional \code{cluster} field in the \code{\link{colData}} containing the cluster identity of each cell.
#'
#' For \code{reducedClusterMNN}, 
#' a \linkS4class{DataFrame} is returned containing the corrected low-dimensional values.
#' This has the same format as the output of \code{\link{reducedMNN}} 
#' but with an additional \code{cluster} field containing the cluster identity of each cell.
#'
#' In both cases, the \code{merge.info} in the \code{\link{metadata}} has the same format as the output of \code{\link{fastMNN}}.
#' However, the \code{pairs} and \code{lost.var} refer to the cluster centroids, not the cells.
#' An additional \code{clusters} DataFrame is provided that can be row-indexed by the values in \code{pairs} 
#' to determine the identity of the clusters in each MNN pair.
#'
#' @details
#' These functions are motivated by the scenario where each batch has been clustered separately
#' (and each cluster has already been annotated with some meaningful biological state).
#' We want to perform correction to examine the relationships between clusters across batches,
#' but in a manner that is more likely to preserve these meaningful within-batch clusters.
#' This is achieved by performing the correction on the cluster centroids and then applying the changes to the per-cell values.
#' 
#' In \code{clusterMNN}, \code{\link{multiBatchPCA}} is called to obtain a low-dimensional representation,
#' and then \code{\link{reducedMNN}} is used to perform the correction on the cluster centroids.
#' Correction of each individual cell is done by applying a Gaussian kernel to the correction vectors for the centroids,
#' where the bandwidth is proportional to the distance between that cell and the centroid of its assigned cluster.
#' This yields a smooth correction function that avoids edge effects at cluster boundaries.
#' 
#' These functions are primarily intended to be used with some user-specified \code{clusters}.
#' However, if \code{clusters=NULL}, we will fill it in by running \code{\link{kmeans}} on the low-dimensional coordinates.
#' This mode can considered as a direct substitute for \code{\link{fastMNN}};
#' the main difference is that \code{clusterMNN} is generally more conservative due to the default \code{k=1} (see comments below),
#' which preserves more within-batch structure but may lead to incomplete merging across batches.
#'
#' @section Theoretical comments:
#' The default setting of \code{k=1} is important as it guarantees that each cluster has no more than one match in another batch.
#' This reduces the risk of inadvertently merging together different clusters from the same batch.
#' By comparison, higher values of \code{k} may result in many-to-one mappings between batches
#' such that the correction will implicitly force different clusters together.
#'
#' The relative performance of \code{clusterMNN} over \code{\link{fastMNN}} as an unsupervised correction method depends - as one might expect - on the clustering:
#' \itemize{
#' \item Unlike \code{\link{fastMNN}}, \code{clusterMNN} solves the kissing problem by working with cluster centroids.
#' This avoids MNNs forming only on the surface of the mass of cells because that mass is collapsed to a single point.
#' However, this only works if the clustering is not so granular that it captures variation along the batch vetor.
#' \item  \code{clusterMNN} may also be more robust thatn \code{\link{fastMNN}} to outlier cells.
#' Such cells form undesirable MNNs with larger clusters of cells in other batches, 
#' preventing those clusters from merging with each other (see Lun, 2019).
#' Provided that these cells do not form separate clusters, their effect on \code{clusterMNN} is mitigated.
#' \item Obviously, if the clustering is not appropriate, \code{clusterMNN} will perform poorly.
#' This is less of an issue in the expected use case where meaningful, user-curated \code{clusters} are provided.
#' The automated k-means step can occasionally be problematic depending on the random initialization,
#' so some inspection of the output \code{cluster} per cell may be warranted if the input \code{clusters} is set to \code{NULL}.
#' }
#' 
#' @author Aaron Lun
#' @examples
#' # Mocking up some data for multiple batches:
#' means <- matrix(rnorm(3000), ncol=3)
#' colnames(means) <- LETTERS[1:3]
#'
#' B1 <- means[,sample(LETTERS[1:3], 500, replace=TRUE)]
#' B1 <- B1 + rnorm(length(B1))
#'
#' B2 <- means[,sample(LETTERS[1:3], 500, replace=TRUE)]
#' B2 <- B2 + rnorm(length(B2)) + rnorm(nrow(B2)) # batch effect.
#'
#' # Applying the correction with some made-up clusters:
#' cluster1 <- kmeans(t(B1), centers=10)$cluster
#' cluster2 <- kmeans(t(B2), centers=10)$cluster
#' out <- clusterMNN(B1, B2, clusters=list(cluster1, cluster2)) 
#'
#' rd <- reducedDim(out, "corrected") 
#' plot(rd[,1], rd[,2], col=out$batch)
#' 
#' @references
#' Lun ATL (2019).
#' A discussion of the known failure points of the fastMNN algorithm.
#' \url{https://marionilab.github.io/FurtherMNN2018/theory/failure.html}
#'
#' @export
#' @importFrom scater .bpNotSharedOrUp
#' @importFrom BiocParallel bpstart bpstop
#' @importFrom utils tail
#' @importFrom BiocNeighbors queryKNN
#' @importFrom S4Vectors DataFrame metadata metadata<-
#' @importFrom BiocGenerics rbind
#' @importFrom igraph make_graph components
clusterMNN <- function(..., batch=NULL, restrict=NULL, clusters=NULL, 
    cos.norm=TRUE, merge.order=NULL, auto.merge=FALSE, min.batch.skip=0,
    subset.row=NULL, correct.all=FALSE, assay.type="logcounts",
    BSPARAM=ExactParam(), BNPARAM=KmknnParam(), BPPARAM=SerialParam()) 
{
    # Setting up the parallelization environment.
    if (.bpNotSharedOrUp(BPPARAM)) {
        bpstart(BPPARAM)
        on.exit(bpstop(BPPARAM), add=TRUE)
    }

    batches <- list(...)
    if (!is.null(batch)) {
        divided <- divideIntoBatches(x=batches[[1]], restrict=restrict[[1]], batch=batch, byrow=FALSE)
        restrict <- divided$restrict
        batches <- divided$batches
        if (!is.function(clusters)) {
            clusters <- split(clusters, batch)
        }
    } 

    if (cos.norm) { 
        l2 <- lapply(batches, cosineNorm, mode="l2norm", subset.row=subset.row, BPPARAM=BPPARAM)
        batches <- mapply(batches, l2, FUN=.apply_cosine_norm, SIMPLIFY=FALSE)
    }

    cluster.out <- .format_clusters(batches=batches, batch=batch, clusters=clusters, 
        restrict=restrict, subset.row=subset.row)

    # PCA is strictly for allowing us to impute genes outside of subset.row,
    # and to allow us to return a low-rank matrix of per-cell expression values.
    # There is no signal:noise trade-off because we ask for the maximum 'd'.
    pca <- do.call(multiBatchPCA, c(cluster.out$centers, 
        list(BSPARAM=BSPARAM, d=sum(vapply(cluster.out$centers, ncol, 0L)) - 1L, 
            get.all.genes=correct.all, subset.row=subset.row)))

    merge.out <- do.call(reducedMNN, c(as.list(pca), 
        list(k=1, merge.order=merge.order, auto.merge=auto.merge, 
            BNPARAM=BNPARAM, BPPARAM=BPPARAM)))

    prop.out <- .propagate_to_cells(batches, pca, merge.out, 
        BNPARAM=BNPARAM, BPPARAM=BPPARAM,
        correct.all=correct.all, subset.row=subset.row)

    output <- .convert_to_SCE(prop.out, pca)
    metadata(output) <- metadata(merge.out)
    metadata(output)$cluster <- DataFrame(cluster=rownames(merge.out), batch=merge.out$batch)
    
    # Identifying clusters of clusters across batches.
    all.pairs <- unlist(metadata(output)$merge.info$pairs)
    g <- make_graph(rbind(all.pairs$left, all.pairs$right), n=nrow(merge.out), directed=FALSE)
    metadata(output)$cluster$meta <- components(g)$membership

    output
}

#' @importFrom DelayedArray DelayedArray colsum
#' @importFrom Matrix t
.format_clusters <- function(batches, batch, clusters, restrict, subset.row=NULL) {
    if (is.null(clusters)) {
        stop("not yet supported")
    } 
    if (length(clusters)!=length(batches)) {
        stop("'...' and 'clusters' should be of the same length")
    }
    for (i in seq_along(clusters)) {
        if (ncol(batches[[i]])!=length(clusters[[i]])) {
            stop("corresponding entries of '...' and 'clusters' should have the same number of cells")
        }
    }

    clusters <- lapply(clusters, as.character)
    centers <- vector("list", length(clusters))

    for (i in seq_along(batches)) {
        B <- DelayedArray(batches[[i]])
        C <- clusters[[i]]
        if (!is.null(curres <- restrict[[i]])) {
            cls <- C[curres]
            mat <- colsum(B[,curres,drop=FALSE], cls)
        } else {
            cls <- C
            mat <- colsum(B, cls)
        }
        centers[[i]] <- t(t(mat)/as.numeric(table(C)[colnames(mat)]))
    }

    names(centers) <- names(clusters) <- names(batches)
    list(centers=centers, clusters=clusters)
}

#' @importFrom stats median
#' @importFrom Matrix colSums crossprod rowSums t
#' @importFrom BiocNeighbors queryKNN
#' @importFrom S4Vectors metadata DataFrame
#' @importFrom utils tail
#' @importFrom scater .subset2index
.propagate_to_cells <- function(batches, pca, after, subset.row, correct.all, BNPARAM, BPPARAM) {
    pca.center <- metadata(pca)$centers
    pca.rotation <- metadata(pca)$rotation

    subset.row <- .subset2index(subset.row, batches[[1]], byrow=TRUE)
    if (correct.all) {
        # Undoing the expansion to all vectors.
        pca.rotation <- pca.rotation[subset.row,,drop=FALSE]
        pca.center <- pca.center[subset.row] 
    }
    pca.adj <- drop(pca.center %*% pca.rotation)

    last <- 0L
    renamed <- vector("list", length(batches))

    for (i in seq_along(batches)) {
        curbatch <- batches[[i]][subset.row,,drop=FALSE]
        curbatch <- crossprod(curbatch, pca.rotation)
        curbatch <- t(t(curbatch) - pca.adj)

        centroids <- pca[[i]]
        indices <- last + seq_len(nrow(centroids))
        last <- tail(indices, 1L)

        corrected <- after$corrected[indices,,drop=FALSE]
        delta <- corrected - centroids
        sigma <- median(queryKNN(query=curbatch, X=centroids, k=1, 
            BNPARAM=BNPARAM, BPPARAM=BPPARAM, get.index=FALSE)$distance[,1])

        # Using a two-pass Gaussian kernel to avoid underflow.
        tbatch <- t(curbatch)
        weights <- matrix(0, nrow(curbatch), nrow(centroids))
        for (j in seq_len(nrow(centroids))) {
            weights[,j] <- -colSums((tbatch - centroids[j,])^2)
        }
        weights <- weights/sigma^2

        top.weight <- weights[cbind(seq_len(nrow(weights)), max.col(weights))]
        norm.weights <- exp(weights - top.weight)
        norm.weights <- norm.weights/rowSums(norm.weights)

        adj <- curbatch
        for (j in seq_len(nrow(centroids))) {
            adj <- adj + outer(norm.weights[,j], delta[j,])
        }
    
        batches[[i]] <- adj
        renamed[[i]] <- rep(after$batch[indices[1]], nrow(curbatch))
    }

    DataFrame(corrected=I(do.call(rbind, batches)), batch=unlist(renamed))
}
